version: '3.8'

services:
  # Redis - Celery broker (DB 0), backend (DB 1), cache (DB 2)
  redis:
    image: redis:7-alpine
    container_name: sentineltranslate-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - sentineltranslate

  # NVIDIA Triton Inference Server
  triton:
    image: nvcr.io/nvidia/tritonserver:24.11-py3
    container_name: sentineltranslate-triton
    command: tritonserver --model-repository=/models --strict-model-config=false --log-verbose=1 --model-control-mode=poll --allow-http=true --strict-readiness=false
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ./triton/empty-models:/models:ro  # Use empty directory until you add actual ONNX models
      # When you have models, change to: ./triton/model-repository:/models:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - sentineltranslate
    # Note: Base config runs CPU-only. For GPU support, use docker-compose.gpu.yml override
    # Note: Uses poll mode to allow dynamic model loading without requiring models at startup

  # NER Microservice (Named Entity Recognition for 20+ languages)
  # Handles spaCy model loading and entity extraction
  ner-service:
    build:
      context: ./ner-service
      dockerfile: Dockerfile
      args:
        - MODEL_SET=dev  # Use dev models for local testing (6 languages)
    container_name: sentineltranslate-ner
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "python3.11", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8081/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - sentineltranslate
    restart: unless-stopped

  # FastAPI Sidecar (synchronous translation gateway)
  # Talks directly to Triton, handles hallucination detection
  sidecar:
    build:
      context: ./sidecar
      dockerfile: Dockerfile
    container_name: sentineltranslate-sidecar
    ports:
      - "8080:8080"
    environment:
      - TRITON_URL=triton:8000
      - NER_SERVICE_URL=http://ner-service:8081
    depends_on:
      triton:
        condition: service_healthy
      ner-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - sentineltranslate
    restart: unless-stopped

  # FastAPI API (batch S3 parquet translation API)
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: sentineltranslate-api
    ports:
      - "8090:8090"
    environment:
      - REDIS_BROKER=redis://redis:6379/0
      - REDIS_BACKEND=redis://redis:6379/1
      # AWS credentials should be provided via environment or IAM role
      - AWS_REGION=${AWS_REGION:-us-east-1}
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - sentineltranslate
    restart: unless-stopped

  # Celery Worker (batch processing orchestrator)
  # Calls sidecar API for translations, caches results in Redis
  worker:
    build:
      context: ./worker
      dockerfile: Dockerfile
    # Note: No container_name when using replicas - Docker Compose auto-generates unique names
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - SIDECAR_URL=http://sidecar:8080
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_CACHE_DB=2
      - CACHE_TTL=86400  # 24 hours
    depends_on:
      redis:
        condition: service_healthy
      sidecar:
        condition: service_healthy
    networks:
      - sentineltranslate
    restart: unless-stopped
    deploy:
      replicas: 2  # Run 2 worker instances for parallel processing

networks:
  sentineltranslate:
    driver: bridge

volumes:
  redis-data:
