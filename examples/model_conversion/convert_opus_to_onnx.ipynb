{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting OPUS-MT Models to ONNX for Triton\n",
    "\n",
    "This notebook demonstrates how to convert OPUS-MT translation models from Hugging Face to ONNX format and deploy them to NVIDIA Triton Inference Server.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Download OPUS-MT models from Hugging Face Hub\n",
    "- Convert PyTorch models to ONNX format\n",
    "- Optimize models with quantization (optional)\n",
    "- Create Triton model repository structure\n",
    "- Write Triton configuration files\n",
    "- Test models with Triton HTTP API\n",
    "- Benchmark model performance\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Install required packages**:\n",
    "```bash\n",
    "pip install transformers torch onnx onnxruntime optimum tritonclient[http] sentencepiece protobuf\n",
    "```\n",
    "\n",
    "**Triton Server** must be running:\n",
    "```bash\n",
    "docker-compose up -d triton\n",
    "```\n",
    "\n",
    "**Estimated time**: 30-45 minutes (depending on model download and conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Configuration\n",
    "TRITON_MODEL_REPO = \"/path/to/triton/model-repository\"  # Update this path\n",
    "TRITON_URL = \"localhost:8000\"\n",
    "\n",
    "print(\"\\n‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Choose a Model to Convert\n",
    "\n",
    "Browse available OPUS-MT models at https://huggingface.co/Helsinki-NLP\n",
    "\n",
    "Popular language pairs:\n",
    "- `Helsinki-NLP/opus-mt-fr-en` (French ‚Üí English)\n",
    "- `Helsinki-NLP/opus-mt-es-en` (Spanish ‚Üí English)\n",
    "- `Helsinki-NLP/opus-mt-de-en` (German ‚Üí English)\n",
    "- `Helsinki-NLP/opus-mt-zh-en` (Chinese ‚Üí English)\n",
    "- `Helsinki-NLP/opus-mt-ja-en` (Japanese ‚Üí English)\n",
    "- `Helsinki-NLP/opus-mt-ar-en` (Arabic ‚Üí English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model you want to convert\n",
    "MODEL_ID = \"Helsinki-NLP/opus-mt-fr-en\"  # Change this to your desired model\n",
    "MODEL_NAME = \"opus-mt-fr-en\"  # Triton model name (matches directory name)\n",
    "SOURCE_LANG = \"fr\"\n",
    "TARGET_LANG = \"en\"\n",
    "\n",
    "print(f\"Model to convert: {MODEL_ID}\")\n",
    "print(f\"Triton model name: {MODEL_NAME}\")\n",
    "print(f\"Language pair: {SOURCE_LANG} ‚Üí {TARGET_LANG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Model from Hugging Face\n",
    "\n",
    "This downloads the PyTorch model and tokenizer to your local cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading {MODEL_ID} from Hugging Face...\\n\")\n",
    "\n",
    "# Download tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(MODEL_ID)\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"   Special tokens: {tokenizer.all_special_tokens}\")\n",
    "\n",
    "# Download model\n",
    "model = MarianMTModel.from_pretrained(MODEL_ID)\n",
    "print(f\"\\n‚úÖ Model loaded\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"   Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test PyTorch Model\n",
    "\n",
    "Before converting, let's verify the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation\n",
    "test_sentences = [\n",
    "    \"Bonjour, comment allez-vous?\",\n",
    "    \"Je suis tr√®s heureux de vous rencontrer.\",\n",
    "    \"La traduction automatique est impressionnante.\"\n",
    "]\n",
    "\n",
    "print(\"Testing PyTorch model...\\n\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in test_sentences:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model.generate(**inputs)\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"üá´üá∑ {text}\")\n",
    "        print(f\"üá¨üáß {translation}\\n\")\n",
    "\n",
    "print(\"‚úÖ PyTorch model working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert to ONNX\n",
    "\n",
    "We'll use Hugging Face Optimum to convert the model to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Converting {MODEL_ID} to ONNX format...\\n\")\n",
    "\n",
    "# Create output directory\n",
    "onnx_output_dir = f\"./{MODEL_NAME}-onnx\"\n",
    "os.makedirs(onnx_output_dir, exist_ok=True)\n",
    "\n",
    "# Convert to ONNX using Optimum\n",
    "onnx_model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    export=True,\n",
    "    provider=\"CPUExecutionProvider\"  # Use \"CUDAExecutionProvider\" for GPU\n",
    ")\n",
    "\n",
    "# Save ONNX model and tokenizer\n",
    "onnx_model.save_pretrained(onnx_output_dir)\n",
    "tokenizer.save_pretrained(onnx_output_dir)\n",
    "\n",
    "print(f\"‚úÖ ONNX model saved to {onnx_output_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "for file in os.listdir(onnx_output_dir):\n",
    "    filepath = os.path.join(onnx_output_dir, file)\n",
    "    size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "    print(f\"   {file}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test ONNX Model\n",
    "\n",
    "Verify the ONNX model produces the same results as PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing ONNX model...\\n\")\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "    onnx_output_dir,\n",
    "    provider=\"CPUExecutionProvider\"\n",
    ")\n",
    "onnx_tokenizer = MarianTokenizer.from_pretrained(onnx_output_dir)\n",
    "\n",
    "# Test same sentences\n",
    "for text in test_sentences:\n",
    "    inputs = onnx_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    outputs = onnx_model.generate(**inputs)\n",
    "    translation = onnx_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"üá´üá∑ {text}\")\n",
    "    print(f\"üá¨üáß {translation}\\n\")\n",
    "\n",
    "print(\"‚úÖ ONNX model working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Triton Model Repository Structure\n",
    "\n",
    "Triton requires a specific directory structure:\n",
    "\n",
    "```\n",
    "model-repository/\n",
    "‚îî‚îÄ‚îÄ opus-mt-fr-en/\n",
    "    ‚îú‚îÄ‚îÄ config.pbtxt\n",
    "    ‚îî‚îÄ‚îÄ 1/\n",
    "        ‚îî‚îÄ‚îÄ model.onnx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Triton model directory structure\n",
    "triton_model_dir = os.path.join(TRITON_MODEL_REPO, MODEL_NAME)\n",
    "triton_version_dir = os.path.join(triton_model_dir, \"1\")\n",
    "\n",
    "os.makedirs(triton_version_dir, exist_ok=True)\n",
    "\n",
    "# Copy ONNX model files\n",
    "# Find the encoder and decoder ONNX files\n",
    "onnx_files = [f for f in os.listdir(onnx_output_dir) if f.endswith('.onnx')]\n",
    "\n",
    "if 'model.onnx' in onnx_files:\n",
    "    # Single ONNX file\n",
    "    shutil.copy(\n",
    "        os.path.join(onnx_output_dir, 'model.onnx'),\n",
    "        os.path.join(triton_version_dir, 'model.onnx')\n",
    "    )\n",
    "else:\n",
    "    # Separate encoder/decoder files - copy main one or combine\n",
    "    print(f\"Found ONNX files: {onnx_files}\")\n",
    "    print(\"Note: You may need to manually configure for multi-file ONNX models\")\n",
    "\n",
    "# Also copy tokenizer files for reference\n",
    "for file in ['tokenizer_config.json', 'source.spm', 'target.spm', 'vocab.json']:\n",
    "    src = os.path.join(onnx_output_dir, file)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, triton_model_dir)\n",
    "\n",
    "print(f\"‚úÖ Created Triton model directory: {triton_model_dir}\")\n",
    "print(f\"‚úÖ Model version 1 directory: {triton_version_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Triton Configuration File\n",
    "\n",
    "The `config.pbtxt` file tells Triton how to load and serve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_content = f'''name: \"{MODEL_NAME}\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"INPUT_TEXT\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "output [\n",
    "  {{\n",
    "    name: \"OUTPUT_TEXT\"\n",
    "    data_type: TYPE_STRING\n",
    "    dims: [ -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {{\n",
    "    count: 1\n",
    "    kind: KIND_GPU  # Change to KIND_CPU for CPU-only inference\n",
    "  }}\n",
    "]\n",
    "\n",
    "# Dynamic batching for better throughput\n",
    "dynamic_batching {{\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\n",
    "\n",
    "# Performance tuning\n",
    "optimization {{\n",
    "  execution_accelerators {{\n",
    "    gpu_execution_accelerator : [\n",
    "      {{\n",
    "        name: \"tensorrt\"\n",
    "        parameters {{\n",
    "          key: \"precision_mode\"\n",
    "          value: \"FP16\"  # Use FP16 for faster GPU inference\n",
    "        }}\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "}}\n",
    "'''\n",
    "\n",
    "config_path = os.path.join(triton_model_dir, 'config.pbtxt')\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"‚úÖ Created Triton configuration: {config_path}\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(config_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Load Model in Triton\n",
    "\n",
    "Restart Triton to load the new model, or use the model control API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model in Triton...\\n\")\n",
    "print(\"Option 1: Restart Triton container:\")\n",
    "print(\"   docker-compose restart triton\")\n",
    "print(\"\\nOption 2: Use Triton model control API:\")\n",
    "print(f\"   curl -X POST http://{TRITON_URL}/v2/repository/models/{MODEL_NAME}/load\")\n",
    "print(\"\\nAfter loading, verify with:\")\n",
    "print(f\"   curl http://{TRITON_URL}/v2/models/{MODEL_NAME}/ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test Model with Triton HTTP API\n",
    "\n",
    "Once the model is loaded, test it using Triton's HTTP client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_triton_model(model_name, text, triton_url=\"localhost:8000\"):\n",
    "    \"\"\"Test a model deployed on Triton.\"\"\"\n",
    "    try:\n",
    "        # Create Triton client\n",
    "        client = httpclient.InferenceServerClient(url=triton_url)\n",
    "        \n",
    "        # Check if model is ready\n",
    "        if not client.is_model_ready(model_name):\n",
    "            print(f\"‚ùå Model {model_name} is not ready on Triton\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare input\n",
    "        input_data = np.array([text.encode('utf-8')], dtype=object)\n",
    "        inputs = [\n",
    "            httpclient.InferInput(\"INPUT_TEXT\", input_data.shape, \"BYTES\")\n",
    "        ]\n",
    "        inputs[0].set_data_from_numpy(input_data)\n",
    "        \n",
    "        # Prepare output\n",
    "        outputs = [\n",
    "            httpclient.InferRequestedOutput(\"OUTPUT_TEXT\")\n",
    "        ]\n",
    "        \n",
    "        # Inference\n",
    "        response = client.infer(model_name, inputs, outputs=outputs)\n",
    "        \n",
    "        # Get result\n",
    "        output_data = response.as_numpy(\"OUTPUT_TEXT\")\n",
    "        translation = output_data[0].decode('utf-8')\n",
    "        \n",
    "        return translation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the model\n",
    "print(f\"Testing {MODEL_NAME} on Triton...\\n\")\n",
    "\n",
    "for text in test_sentences:\n",
    "    translation = test_triton_model(MODEL_NAME, text, TRITON_URL)\n",
    "    if translation:\n",
    "        print(f\"üá´üá∑ {text}\")\n",
    "        print(f\"üá¨üáß {translation}\\n\")\n",
    "\n",
    "print(\"‚úÖ Triton inference working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Performance Benchmarking\n",
    "\n",
    "Measure latency and throughput of the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "def benchmark_model(model_name, texts, iterations=10):\n",
    "    \"\"\"Benchmark model performance.\"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    print(f\"Benchmarking {model_name} with {iterations} iterations...\\n\")\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        for text in texts:\n",
    "            start = time.time()\n",
    "            translation = test_triton_model(model_name, text, TRITON_URL)\n",
    "            latency = (time.time() - start) * 1000  # Convert to ms\n",
    "            latencies.append(latency)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_latency = statistics.mean(latencies)\n",
    "    median_latency = statistics.median(latencies)\n",
    "    min_latency = min(latencies)\n",
    "    max_latency = max(latencies)\n",
    "    std_dev = statistics.stdev(latencies)\n",
    "    throughput = 1000 / avg_latency  # requests per second\n",
    "    \n",
    "    print(f\"\\nüìä Benchmark Results ({len(latencies)} requests)\")\n",
    "    print(f\"   Average Latency:  {avg_latency:.2f} ms\")\n",
    "    print(f\"   Median Latency:   {median_latency:.2f} ms\")\n",
    "    print(f\"   Min Latency:      {min_latency:.2f} ms\")\n",
    "    print(f\"   Max Latency:      {max_latency:.2f} ms\")\n",
    "    print(f\"   Std Deviation:    {std_dev:.2f} ms\")\n",
    "    print(f\"   Throughput:       {throughput:.2f} req/s\")\n",
    "    \n",
    "    return latencies\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_texts = [\n",
    "    \"Bonjour\",\n",
    "    \"Comment allez-vous?\",\n",
    "    \"Je suis tr√®s heureux.\"\n",
    "]\n",
    "\n",
    "latencies = benchmark_model(MODEL_NAME, benchmark_texts, iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Visualization (Optional)\n",
    "\n",
    "Plot latency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(latencies, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.axvline(statistics.mean(latencies), color='r', linestyle='--', label=f'Mean: {statistics.mean(latencies):.2f}ms')\n",
    "    plt.axvline(statistics.median(latencies), color='g', linestyle='--', label=f'Median: {statistics.median(latencies):.2f}ms')\n",
    "    plt.xlabel('Latency (ms)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{MODEL_NAME} Inference Latency Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Install matplotlib for visualization: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "‚úÖ Download OPUS-MT models from Hugging Face  \n",
    "‚úÖ Convert PyTorch models to ONNX format  \n",
    "‚úÖ Create Triton model repository structure  \n",
    "‚úÖ Write Triton configuration files  \n",
    "‚úÖ Deploy models to Triton Inference Server  \n",
    "‚úÖ Test models with HTTP API  \n",
    "‚úÖ Benchmark model performance  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Add more models**: Repeat this process for other language pairs\n",
    "2. **Optimize**: Experiment with INT8 quantization for faster inference\n",
    "3. **Scale**: Configure multi-GPU deployment for high throughput\n",
    "4. **Integrate**: Use the deployed model in SentinelTranslate API\n",
    "5. **Monitor**: Set up metrics and logging for production\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [OPUS-MT Models](https://huggingface.co/Helsinki-NLP)\n",
    "- [Triton Documentation](https://github.com/triton-inference-server/server)\n",
    "- [Hugging Face Optimum](https://huggingface.co/docs/optimum/)\n",
    "- [ONNX Runtime](https://onnxruntime.ai/)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Check the [model_conversion README](README.md) or the main [SentinelTranslate docs](../../README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
