{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Translation with S3 Parquet Files\n",
    "\n",
    "This notebook demonstrates how to perform large-scale batch translation using SentinelTranslate's S3-based parquet processing API.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Create parquet files with multilingual data\n",
    "- Upload data to AWS S3\n",
    "- Submit batch translation jobs via API\n",
    "- Monitor job progress and status\n",
    "- Download and analyze translated results\n",
    "- Handle errors and retry strategies\n",
    "- Optimize for throughput and cost\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Python packages**:\n",
    "```bash\n",
    "pip install pandas pyarrow boto3 requests matplotlib\n",
    "```\n",
    "\n",
    "**AWS credentials** (set as environment variables):\n",
    "```bash\n",
    "export AWS_ACCESS_KEY_ID='your-key'\n",
    "export AWS_SECRET_ACCESS_KEY='your-secret'\n",
    "export AWS_DEFAULT_REGION='us-east-1'\n",
    "```\n",
    "\n",
    "**Services running**:\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "**Estimated time**: 30-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configuration\n",
    "BATCH_API_URL = \"http://localhost:8090\"\n",
    "S3_BUCKET = \"my-translation-bucket\"  # Update with your bucket name\n",
    "S3_INPUT_PREFIX = \"input/\"\n",
    "S3_OUTPUT_PREFIX = \"output/\"\n",
    "\n",
    "print(\"âœ… Imports successful\")\n",
    "print(f\"ðŸ“¦ Batch API: {BATCH_API_URL}\")\n",
    "print(f\"ðŸª£ S3 Bucket: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Services are Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check batch API health\n",
    "try:\n",
    "    response = requests.get(f\"{BATCH_API_URL}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ… Batch API is healthy\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Batch API returned status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Batch API not reachable: {e}\")\n",
    "    print(\"   Please start services: docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Sample Data\n",
    "\n",
    "Let's create a realistic dataset with multilingual customer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample multilingual data\n",
    "np.random.seed(42)\n",
    "\n",
    "# French reviews\n",
    "french_reviews = [\n",
    "    \"Excellent produit, trÃ¨s satisfait de mon achat.\",\n",
    "    \"La qualitÃ© est mÃ©diocre pour le prix.\",\n",
    "    \"Livraison rapide et service client rÃ©actif.\",\n",
    "    \"Je recommande vivement cette entreprise.\",\n",
    "    \"Le produit ne correspond pas Ã  la description.\",\n",
    "    \"TrÃ¨s bon rapport qualitÃ©-prix.\",\n",
    "    \"DÃ©Ã§u par la qualitÃ© du matÃ©riau.\",\n",
    "    \"Service aprÃ¨s-vente impeccable.\",\n",
    "    \"Le dÃ©lai de livraison Ã©tait trop long.\",\n",
    "    \"Produit conforme Ã  mes attentes.\"\n",
    "]\n",
    "\n",
    "# Spanish reviews\n",
    "spanish_reviews = [\n",
    "    \"Excelente producto, muy satisfecho con la compra.\",\n",
    "    \"La calidad es mediocre para el precio.\",\n",
    "    \"Entrega rÃ¡pida y servicio al cliente receptivo.\",\n",
    "    \"Recomiendo encarecidamente esta empresa.\",\n",
    "    \"El producto no coincide con la descripciÃ³n.\",\n",
    "    \"Muy buena relaciÃ³n calidad-precio.\",\n",
    "    \"Decepcionado con la calidad del material.\",\n",
    "    \"Servicio postventa impecable.\",\n",
    "    \"El tiempo de entrega fue demasiado largo.\",\n",
    "    \"Producto conforme a mis expectativas.\"\n",
    "]\n",
    "\n",
    "# German reviews\n",
    "german_reviews = [\n",
    "    \"Ausgezeichnetes Produkt, sehr zufrieden mit dem Kauf.\",\n",
    "    \"Die QualitÃ¤t ist mittelmÃ¤ÃŸig fÃ¼r den Preis.\",\n",
    "    \"Schnelle Lieferung und reaktionsschneller Kundenservice.\",\n",
    "    \"Ich empfehle dieses Unternehmen wÃ¤rmstens.\",\n",
    "    \"Das Produkt entspricht nicht der Beschreibung.\",\n",
    "    \"Sehr gutes Preis-Leistungs-VerhÃ¤ltnis.\",\n",
    "    \"EnttÃ¤uscht von der MaterialqualitÃ¤t.\",\n",
    "    \"Einwandfreier Kundenservice.\",\n",
    "    \"Die Lieferzeit war zu lang.\",\n",
    "    \"Produkt entspricht meinen Erwartungen.\"\n",
    "]\n",
    "\n",
    "# Combine all reviews\n",
    "all_reviews = (\n",
    "    [(text, 'fr') for text in french_reviews] +\n",
    "    [(text, 'es') for text in spanish_reviews] +\n",
    "    [(text, 'de') for text in german_reviews]\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'review_id': range(1, len(all_reviews) + 1),\n",
    "    'text': [r[0] for r in all_reviews],\n",
    "    'source_lang': [r[1] for r in all_reviews],\n",
    "    'rating': np.random.randint(1, 6, len(all_reviews)),\n",
    "    'product_category': np.random.choice(\n",
    "        ['Electronics', 'Clothing', 'Home', 'Books', 'Sports'],\n",
    "        len(all_reviews)\n",
    "    ),\n",
    "    'created_at': [\n",
    "        datetime.now() - timedelta(days=np.random.randint(0, 365))\n",
    "        for _ in range(len(all_reviews))\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"âœ… Created sample dataset with {len(df)} reviews\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(df['source_lang'].value_counts())\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save Data to Parquet\n",
    "\n",
    "Parquet is a columnar storage format that's efficient for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet files (one per language for this example)\n",
    "parquet_files = {}\n",
    "\n",
    "for lang in df['source_lang'].unique():\n",
    "    lang_df = df[df['source_lang'] == lang]\n",
    "    filename = f'reviews_{lang}.parquet'\n",
    "    lang_df.to_parquet(filename, index=False, engine='pyarrow')\n",
    "    parquet_files[lang] = filename\n",
    "    print(f\"âœ… Saved {filename} ({len(lang_df)} rows)\")\n",
    "\n",
    "# Also save combined file\n",
    "combined_file = 'reviews_combined.parquet'\n",
    "df.to_parquet(combined_file, index=False, engine='pyarrow')\n",
    "print(f\"âœ… Saved {combined_file} ({len(df)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Upload to S3\n",
    "\n",
    "Upload parquet files to S3 for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create bucket if it doesn't exist (optional)\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "    print(f\"âœ… Bucket {S3_BUCKET} exists\")\n",
    "except:\n",
    "    print(f\"Creating bucket {S3_BUCKET}...\")\n",
    "    s3_client.create_bucket(Bucket=S3_BUCKET)\n",
    "    print(f\"âœ… Created bucket {S3_BUCKET}\")\n",
    "\n",
    "# Upload files\n",
    "s3_files = {}\n",
    "for lang, local_file in parquet_files.items():\n",
    "    s3_key = f\"{S3_INPUT_PREFIX}{local_file}\"\n",
    "    s3_client.upload_file(local_file, S3_BUCKET, s3_key)\n",
    "    s3_files[lang] = s3_key\n",
    "    print(f\"âœ… Uploaded {local_file} to s3://{S3_BUCKET}/{s3_key}\")\n",
    "\n",
    "# Upload combined file\n",
    "combined_s3_key = f\"{S3_INPUT_PREFIX}{combined_file}\"\n",
    "s3_client.upload_file(combined_file, S3_BUCKET, combined_s3_key)\n",
    "print(f\"âœ… Uploaded {combined_file} to s3://{S3_BUCKET}/{combined_s3_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Helper Functions for Batch Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_batch_job(s3_bucket: str, s3_key: str, text_column: str,\n",
    "                     source_lang: str, target_lang: str = 'en',\n",
    "                     output_s3_key: str = None, id_column: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"Submit a batch translation job.\"\"\"\n",
    "    payload = {\n",
    "        's3_bucket': s3_bucket,\n",
    "        's3_key': s3_key,\n",
    "        'text_column': text_column,\n",
    "        'source_lang': source_lang,\n",
    "        'target_lang': target_lang\n",
    "    }\n",
    "    \n",
    "    if output_s3_key:\n",
    "        payload['output_s3_key'] = output_s3_key\n",
    "    if id_column:\n",
    "        payload['id_column'] = id_column\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{BATCH_API_URL}/batch/translate\", json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to submit job: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_job_status(job_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Check status of a batch job.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{BATCH_API_URL}/batch/status/{job_id}\", timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to check status: {e}\")\n",
    "        return None\n",
    "\n",
    "def wait_for_completion(job_id: str, poll_interval: int = 5, max_wait: int = 600) -> Dict[str, Any]:\n",
    "    \"\"\"Poll until job completes or times out.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time) < max_wait:\n",
    "        status = check_job_status(job_id)\n",
    "        \n",
    "        if not status:\n",
    "            time.sleep(poll_interval)\n",
    "            continue\n",
    "        \n",
    "        print(f\"Job {job_id[:8]}... status: {status['status']}\")\n",
    "        \n",
    "        if status['status'] in ['SUCCESS', 'FAILURE']:\n",
    "            return status\n",
    "        \n",
    "        time.sleep(poll_interval)\n",
    "    \n",
    "    print(f\"â±ï¸  Timeout after {max_wait}s\")\n",
    "    return None\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Submit Batch Translation Jobs\n",
    "\n",
    "Submit a job for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit jobs for each language\n",
    "jobs = {}\n",
    "\n",
    "for lang, s3_key in s3_files.items():\n",
    "    output_key = f\"{S3_OUTPUT_PREFIX}reviews_{lang}_translated.parquet\"\n",
    "    \n",
    "    print(f\"\\nSubmitting job for {lang}...\")\n",
    "    result = submit_batch_job(\n",
    "        s3_bucket=S3_BUCKET,\n",
    "        s3_key=s3_key,\n",
    "        text_column='text',\n",
    "        source_lang=lang,\n",
    "        target_lang='en',\n",
    "        output_s3_key=output_key,\n",
    "        id_column='review_id'\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        jobs[lang] = {\n",
    "            'job_id': result['job_id'],\n",
    "            'output_key': output_key,\n",
    "            'submitted_at': datetime.now()\n",
    "        }\n",
    "        print(f\"âœ… Job submitted: {result['job_id']}\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed to submit job for {lang}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Submitted {len(jobs)} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Monitor Job Progress\n",
    "\n",
    "Wait for all jobs to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for all jobs to complete\n",
    "print(\"Waiting for jobs to complete...\\n\")\n",
    "\n",
    "results = {}\n",
    "for lang, job_info in jobs.items():\n",
    "    job_id = job_info['job_id']\n",
    "    print(f\"\\nMonitoring {lang} job ({job_id[:8]}...)\")\n",
    "    \n",
    "    final_status = wait_for_completion(job_id, poll_interval=5, max_wait=300)\n",
    "    \n",
    "    if final_status and final_status['status'] == 'SUCCESS':\n",
    "        print(f\"âœ… {lang} translation completed successfully\")\n",
    "        print(f\"   Output: {final_status['result']['output_location']}\")\n",
    "        print(f\"   Total rows: {final_status['result']['total_rows']}\")\n",
    "        print(f\"   Successful: {final_status['result']['successful_translations']}\")\n",
    "        print(f\"   Failed: {final_status['result']['failed_rows']}\")\n",
    "        results[lang] = final_status\n",
    "    else:\n",
    "        print(f\"âŒ {lang} translation failed or timed out\")\n",
    "        if final_status:\n",
    "            print(f\"   Error: {final_status.get('error', 'Unknown')}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ {len(results)} of {len(jobs)} jobs completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download translated files\n",
    "translated_dfs = {}\n",
    "\n",
    "for lang, result in results.items():\n",
    "    output_location = result['result']['output_location']\n",
    "    # Parse s3://bucket/key\n",
    "    bucket, key = output_location.replace('s3://', '').split('/', 1)\n",
    "    \n",
    "    local_file = f'reviews_{lang}_translated.parquet'\n",
    "    \n",
    "    print(f\"Downloading {lang} results...\")\n",
    "    s3_client.download_file(bucket, key, local_file)\n",
    "    \n",
    "    # Read parquet\n",
    "    df_translated = pd.read_parquet(local_file)\n",
    "    translated_dfs[lang] = df_translated\n",
    "    \n",
    "    print(f\"âœ… Downloaded {local_file} ({len(df_translated)} rows)\")\n",
    "    print(f\"   Columns: {list(df_translated.columns)}\")\n",
    "    print(f\"\\nSample translations:\")\n",
    "    print(df_translated[['text', 'translation']].head(3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Quality Analysis\n",
    "\n",
    "Analyze translation quality and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all translated data\n",
    "all_translated = pd.concat(translated_dfs.values(), ignore_index=True)\n",
    "\n",
    "print(f\"ðŸ“Š Translation Summary\\n\")\n",
    "print(f\"Total rows processed: {len(all_translated)}\")\n",
    "print(f\"Successful translations: {len(all_translated[all_translated['translation'].notna()])}\")\n",
    "print(f\"Failed translations: {len(all_translated[all_translated['translation'].isna()])}\")\n",
    "\n",
    "# Language breakdown\n",
    "print(f\"\\nBy language:\")\n",
    "for lang in all_translated['source_lang'].unique():\n",
    "    lang_df = all_translated[all_translated['source_lang'] == lang]\n",
    "    success_count = len(lang_df[lang_df['translation'].notna()])\n",
    "    print(f\"  {lang}: {success_count}/{len(lang_df)} successful\")\n",
    "\n",
    "# Average text length\n",
    "all_translated['text_length'] = all_translated['text'].str.len()\n",
    "all_translated['translation_length'] = all_translated['translation'].str.len()\n",
    "\n",
    "print(f\"\\nText statistics:\")\n",
    "print(f\"  Average source length: {all_translated['text_length'].mean():.0f} chars\")\n",
    "print(f\"  Average translation length: {all_translated['translation_length'].mean():.0f} chars\")\n",
    "print(f\"  Length ratio: {(all_translated['translation_length'] / all_translated['text_length']).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Plot translation length comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Length distribution\n",
    "    all_translated.boxplot(column=['text_length', 'translation_length'], ax=ax1)\n",
    "    ax1.set_title('Text Length Distribution')\n",
    "    ax1.set_ylabel('Characters')\n",
    "    ax1.set_xticklabels(['Source', 'Translation'])\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Success rate by language\n",
    "    success_by_lang = all_translated.groupby('source_lang').apply(\n",
    "        lambda x: len(x[x['translation'].notna()]) / len(x) * 100\n",
    "    )\n",
    "    success_by_lang.plot(kind='bar', ax=ax2, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "    ax2.set_title('Success Rate by Language')\n",
    "    ax2.set_ylabel('Success Rate (%)')\n",
    "    ax2.set_xlabel('Source Language')\n",
    "    ax2.set_ylim([0, 100])\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Install matplotlib for visualizations: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… How to prepare multilingual data in parquet format  \n",
    "âœ… Upload data to AWS S3  \n",
    "âœ… Submit batch translation jobs via API  \n",
    "âœ… Monitor job progress and handle completion  \n",
    "âœ… Download and analyze translated results  \n",
    "âœ… Measure translation quality and performance  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Scale up**: Process larger datasets (10K+ rows)\n",
    "2. **Optimize**: Tune worker count for better throughput\n",
    "3. **Integrate**: Add batch translation to your data pipelines\n",
    "4. **Monitor**: Set up alerts for job failures\n",
    "5. **Quality**: Implement custom quality checks\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Batch Translation README](README.md)\n",
    "- [SentinelTranslate Documentation](../../README.md)\n",
    "- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)\n",
    "- [Parquet Format](https://parquet.apache.org/)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Open an issue on GitHub or check the main documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
